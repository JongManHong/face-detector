{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluation for FER2013.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "\n",
    "import fer2013_2\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('eval_dir', './tmp/fer2013_eval',\n",
    "                           \"\"\"Directory where to write event logs.\"\"\")\n",
    "tf.app.flags.DEFINE_string('eval_data', 'test',\n",
    "                           \"\"\"Either 'test' or 'train_eval'.\"\"\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir', './',\n",
    "                           \"\"\"Directory where to read model checkpoints.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('eval_interval_secs', 60 * 5,\n",
    "                            \"\"\"How often to run the eval.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_examples', 3589,\n",
    "                            \"\"\"Number of examples to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('run_once', True,\n",
    "                         \"\"\"Whether to run eval only once.\"\"\")\n",
    "\n",
    "\n",
    "def eval_once(saver, summary_writer, logits, labels, top_k_op, summary_op):\n",
    "  # print(\"Called eval_once ...\")\n",
    "  \"\"\"Run Eval once.\n",
    "  Args:\n",
    "    saver: Saver.\n",
    "    summary_writer: Summary writer.\n",
    "    top_k_op: Top K op.\n",
    "    summary_op: Summary op.\n",
    "  \"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      # Restores from checkpoint\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      print(\"Checkpoint file path:\", ckpt.model_checkpoint_path)\n",
    "      # Assuming model_checkpoint_path looks something like:\n",
    "      #   /my-favorite-path/fer2013_train/model.ckpt-0,\n",
    "      # extract global_step from it.\n",
    "      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "    else:\n",
    "      print('No checkpoint file found')\n",
    "      return\n",
    "\n",
    "    # Start the queue runners.\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "      threads = []\n",
    "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
    "        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
    "                                         start=True))\n",
    "\n",
    "      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_input_size))\n",
    "      true_count = 0  # Counts the number of correct predictions.\n",
    "      total_sample_count = num_iter * FLAGS.batch_input_size\n",
    "      step = 0\n",
    "      time.sleep(1)\n",
    "\n",
    "      # print(\"step = %d, num_iter = %d  \" % (step, num_iter))\n",
    "\n",
    "      emotion_dict = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Sad'}\n",
    "\n",
    "      while step < num_iter and not coord.should_stop():\n",
    "        # print(\"Inside while ...\")\n",
    "        result1, result2  = sess.run([logits, labels])\n",
    "        #label = sess.run(labels)\n",
    "        # print('Step:', step, 'result',result1, 'Label:', result2)\n",
    "        c = sess.run(tf.arg_max(result1, 1))\n",
    "        \n",
    "        print(\"-----------------------------------------------------\")\n",
    "        print('LABEL FOR INPUT IMAGE:', result1, '->', c, emotion_dict[c[0]])\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        step += 1\n",
    "        return\n",
    "\n",
    "      # print(\"Exited while! Next...\")\n",
    "\n",
    "      # Compute precision @ 1.\n",
    "      precision = true_count / step\n",
    "      # print('Summary -- Step:', step, 'Accurcy:',true_count * 100.0 / step * 1.0, )\n",
    "      # print('%s: total:%d true:%d precision @ 1 = %.3f' % (datetime.now(), total_sample_count, true_count, precision))\n",
    "\n",
    "    except Exception as e:  # pylint: disable=broad-except\n",
    "      coord.request_stop(e)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads, stop_grace_period_secs=10)\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "  \"\"\"Eval FER2013 for a number of steps.\"\"\"\n",
    "\n",
    "  img = plt.imread('saved_frame2.png')\n",
    "    \n",
    "  plt.imshow(img) \n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    # Get images and labels for FER2013.\n",
    "    eval_data = FLAGS.eval_data == 'test'\n",
    "    # images, labels = fer2013.inputs(eval_data=eval_data)\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "   \n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "\n",
    "    distorted_image = tf.image.resize_image_with_crop_or_pad(img, 32, 32)\n",
    "    distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "    distorted_image = tf.image.random_contrast(distorted_image, lower=0.2, upper=1.8)\n",
    "    float_image = tf.image.per_image_whitening(distorted_image)  \n",
    "    float_image = tf.reshape(float_image, [1, 32, 32, 1])\n",
    "    \n",
    "    print(float_image)\n",
    "    \n",
    "    logits = fer2013_2.inference(float_image)\n",
    "\n",
    "    # Calculate predictions.\n",
    "    labels = tf.reshape(tf.constant(12), [1])\n",
    "    \n",
    "    print(labels)\n",
    "    \n",
    "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
    "\n",
    "    # Restore the moving average version of the learned variables for eval.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        fer2013_2.MOVING_AVERAGE_DECAY)\n",
    "    variables_to_restore = variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    \n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    graph_def = tf.get_default_graph().as_graph_def()\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,\n",
    "                                            graph_def=graph_def)\n",
    "\n",
    "    while True:\n",
    "      eval_once(saver, summary_writer, logits, labels, top_k_op, summary_op)\n",
    "\n",
    "      if FLAGS.run_once:\n",
    "        break\n",
    "      time.sleep(FLAGS.eval_interval_secs)\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "  if tf.gfile.Exists(FLAGS.eval_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.eval_dir)\n",
    "  evaluate()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#  tf.app.run()    \n",
    "    \n",
    "    import cv2\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt.xml')\n",
    "\n",
    "    if face_cascade.empty():\n",
    "        raise IOError('Unable to load the face cascade classifier xml file')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    scaling_factor = 1\n",
    "\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "#        frame = cv2.resize(frame, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        face_rects = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "        rois = frame\n",
    "    \n",
    "        for (x,y,w,h) in face_rects:\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 3)\n",
    "            rois = frame[y:y+h, x:x+w]\n",
    "\n",
    "        cv2.imshow('Face Detector', frame)\n",
    "    \n",
    "        c = cv2.waitKey(1)\n",
    "        \n",
    "        if c == 27:\n",
    "            break \n",
    "        cv2.imshow('Face Detector_roi', rois)\n",
    "   \n",
    "        if c == ord('s'):\n",
    "            frame_roi = cv2.resize(rois, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            cv2.imshow('captured_roi', frame_roi)\n",
    "            \n",
    "            \n",
    "            cv2.imwrite('saved_frame2.png', frame_roi)\n",
    "        \n",
    "                      \n",
    "            tf.app.run()\n",
    "            \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
